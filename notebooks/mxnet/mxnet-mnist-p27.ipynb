{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"skdata.mnist.dataset\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Accuracy thus far 0.919571314103\n",
      "Starting epoch 1\n",
      "Accuracy thus far 0.937900641026\n",
      "Starting epoch 2\n",
      "Accuracy thus far 0.949018429487\n",
      "Starting epoch 3\n",
      "Accuracy thus far 0.954026442308\n",
      "Starting epoch 4\n",
      "Accuracy thus far 0.963942307692\n",
      "Starting epoch 5\n",
      "Accuracy thus far 0.969851762821\n",
      "Starting epoch 6\n",
      "Accuracy thus far 0.97125400641\n",
      "Starting epoch 7\n",
      "Accuracy thus far 0.974459134615\n",
      "Starting epoch 8\n",
      "Accuracy thus far 0.977063301282\n",
      "Starting epoch 9\n",
      "Accuracy thus far 0.977864583333\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "# First, the symbol needs to be defined\n",
    "data = mx.sym.Variable(\"data\") # input features, mxnet commonly calls this 'data'\n",
    "label = mx.sym.Variable(\"softmax_label\")\n",
    "\n",
    "# One can either manually specify all the inputs to ops (data, weight and bias)\n",
    "w1 = mx.sym.Variable(\"weight1\")\n",
    "b1 = mx.sym.Variable(\"bias1\")\n",
    "l1 = mx.sym.FullyConnected(data=data, num_hidden=128, name=\"layer1\", weight=w1, bias=b1)\n",
    "a1 = mx.sym.Activation(data=l1, act_type=\"relu\", name=\"act1\")\n",
    "\n",
    "# Or let MXNet automatically create the needed arguments to ops\n",
    "l2 = mx.sym.FullyConnected(data=a1, num_hidden=10, name=\"layer2\")\n",
    "\n",
    "# Create some loss symbol\n",
    "cost_classification = mx.sym.SoftmaxOutput(data=l2, label=label)\n",
    "\n",
    "# Bind an executor of a given batch size to do forward pass and get gradients\n",
    "batch_size = 128\n",
    "input_shapes = {\"data\": (batch_size, 28*28), \"softmax_label\": (batch_size, )}\n",
    "executor = cost_classification.simple_bind(ctx=mx.gpu(0),\n",
    "                                           grad_req='write',\n",
    "                                           **input_shapes)\n",
    "# The above executor computes gradients. When evaluating test data we don't need this.\n",
    "# We want this executor to share weights with the above one, so we will use bind\n",
    "# (instead of simple_bind) and use the other executor's arguments.\n",
    "executor_test = cost_classification.bind(ctx=mx.gpu(0),\n",
    "                                         grad_req='null',\n",
    "                                         args=executor.arg_arrays)\n",
    "\n",
    "# initialize the weights\n",
    "for r in executor.arg_arrays:\n",
    "    r[:] = np.random.randn(*r.shape)*0.02\n",
    "\n",
    "# Using skdata to get mnist data. This is for portability. Can sub in any data loading you like.\n",
    "from skdata.mnist.views import OfficialVectorClassification\n",
    "\n",
    "data = OfficialVectorClassification()\n",
    "trIdx = data.sel_idxs[:]\n",
    "teIdx = data.val_idxs[:]\n",
    "for epoch in range(10):\n",
    "  print \"Starting epoch\", epoch\n",
    "  np.random.shuffle(trIdx)\n",
    "\n",
    "  for x in range(0, len(trIdx), batch_size):\n",
    "    # extract a batch from mnist\n",
    "    batchX = data.all_vectors[trIdx[x:x+batch_size]]\n",
    "    batchY = data.all_labels[trIdx[x:x+batch_size]]\n",
    "\n",
    "    # our executor was bound to 128 size. Throw out non matching batches.\n",
    "    if batchX.shape[0] != batch_size:\n",
    "        continue\n",
    "    # Store batch in executor 'data'\n",
    "    executor.arg_dict['data'][:] = batchX / 255.\n",
    "    # Store label's in 'softmax_label'\n",
    "    executor.arg_dict['softmax_label'][:] = batchY\n",
    "    executor.forward()\n",
    "    executor.backward()\n",
    "\n",
    "    # do weight updates in imperative\n",
    "    for pname, W, G in zip(cost_classification.list_arguments(), executor.arg_arrays, executor.grad_arrays):\n",
    "        # Don't update inputs\n",
    "        # MXNet makes no distinction between weights and data.\n",
    "        if pname in ['data', 'softmax_label']:\n",
    "            continue\n",
    "        # what ever fancy update to modify the parameters\n",
    "        W[:] = W - G * .001\n",
    "\n",
    "  # Evaluation at each epoch\n",
    "  num_correct = 0\n",
    "  num_total = 0\n",
    "  for x in range(0, len(teIdx), batch_size):\n",
    "    batchX = data.all_vectors[teIdx[x:x+batch_size]]\n",
    "    batchY = data.all_labels[teIdx[x:x+batch_size]]\n",
    "    if batchX.shape[0] != batch_size:\n",
    "        continue\n",
    "    # use the test executor as we don't care about gradients\n",
    "    executor_test.arg_dict['data'][:] = batchX / 255.\n",
    "    executor_test.forward()\n",
    "    num_correct += sum(batchY == np.argmax(executor_test.outputs[0].asnumpy(), axis=1))\n",
    "    num_total += len(batchY)\n",
    "  print \"Accuracy thus far\", num_correct / float(num_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p27)",
   "language": "python",
   "name": "conda_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
